import numpy as np
import matplotlib.pyplot as plt
import gym

from gym import spaces
from gym.utils import seeding


class NavigationObsEnv(gym.Env):
	"""Simple 2D navigation with obstacle

	At each time step, the 2D agent takes an action (its velocity, clipped in
	[-0.1, 0.1]), and receives a penalty equal to its L2 distance to the goal 
	position (ie. the reward is `-distance`). The 2D navigation tasks are 
	generated by sampling goal positions from the uniform distribution 
	on [-0.5, 0.5]^2.

	"""
	def __init__(self, task={}):
		super(NavigationObsEnv, self).__init__()

		# define observation dimension
		self.action_lim = 0.04
		self.state_dim = 2
		self.observation_space = spaces.Box(low=-np.inf, high=np.inf,
			shape=(self.state_dim,), dtype=np.float32)
		self.action_space = spaces.Box(low=-self.action_lim, high=self.action_lim, shape=(self.state_dim,), dtype=np.float32)

		# Extract task info #!
		self._task = task
		self._goal = task.get('goal', np.zeros(2, dtype=np.float32))
		self._obs_loc = task.get('obs_loc', np.zeros(2, dtype=np.float32))
		self._obs_radius = task.get('obs_radius', np.zeros(1, dtype=np.float32))
		self._obs_buffer = 0.1 # no cost if outside buffer
		self._goal_thres = 0.02

		# self._state = np.zeros(self.state_dim, dtype=np.float32)
		self.seed()

	def seed(self, seed=None):
		self.np_random, seed = seeding.np_random(seed)
		return [seed]

	def reset_task(self, task):
		self._task = task
		self._goal = task['goal']
		self._obs_loc = task['obs_loc']
		self._obs_radius = task['obs_radius']

	def reset(self, env=True):
		self._state = np.zeros(self.state_dim, dtype=np.float32)
		# self._state[2:] = 1
		return self._state

	def step(self, action):
		action = np.clip(action, -self.action_lim, self.action_lim)
		assert self.action_space.contains(action)

		# Dynamics
		self._state[0:2] = self._state[0:2] + action[0:2]

		# Reward: goal and obstacle
		x_goal = self._state[0] - self._goal[0]
		y_goal = self._state[1] - self._goal[1]
		x_obs = self._state[0] - self._obs_loc[0]
		y_obs = self._state[1] - self._obs_loc[1]
		dist_to_goal = np.sqrt(x_goal ** 2 + y_goal ** 2)
		reward_goal = -dist_to_goal

		dist_to_obs_center = np.sqrt(x_obs ** 2 + y_obs ** 2)
		# print(dist_to_obs_center, self._goal, self._obs_loc, self._obs_radius)
		if dist_to_obs_center < self._obs_radius:
			reward_obs = -1
		elif dist_to_obs_center < (self._obs_radius+self._obs_buffer):
			dist_to_obs_boundary = dist_to_obs_center-self._obs_radius
			reward_obs = -dist_to_obs_boundary/self._obs_buffer
		else:
			reward_obs = 0
		reward = reward_goal + reward_obs
		done = (dist_to_goal < self._goal_thres)
		return self._state, reward, done, {'task': self._task}

	################ Not used in episode ################

	def sample_tasks(self, num_tasks, uniform=False):

		# this is to ensure that the goal interation is fixed
		# if self.first_time_flag == 0:
		# radius = 1.0
		# if uniform:
		# 	angle = np.linspace(0, np.pi, num_tasks)
		# else:
		# 	angle = np.random.uniform(0, np.pi, size=(num_tasks,))
		# xpos = radius*np.cos(angle)
		# ypos = radius*np.sin(angle)
		# self.goals = np.vstack((xpos, ypos)).T
		goal = [1.0, 0.0]	
		obs_loc = [0.5, 0.0]
		obs_radius = 0.1	
		# tasks = [{'goal': goal} for goal in self.goals]
		tasks = [{'goal': goal, 'obs_loc': obs_loc, 'obs_radius': obs_radius} for _ in range(num_tasks)]
		return tasks


if __name__ == '__main__':

	# Test single environment in GUI
	env = NavigationObsEnv()
	obs = env.reset()
	
	# fig = plt.figure()
	# plt.imshow(obs, cmap='Greys', origin='lower')
	# plt.xlabel('y')
	# plt.ylabel('z')
	# plt.show()

	for t in range(100):
		action = 16
		obs, reward, done, _ = env.step(action)
		print(reward, done)
		plt.imshow(np.flip(np.swapaxes(obs, 0, 1), 1), cmap='Greys', origin='lower')
		plt.show()    # Default is a blocking call
