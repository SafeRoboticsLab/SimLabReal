# setting: vanilla-normal
# method: PAC-Perf, prior
env:
  venv_seed: 0
  name: vanilla
  dataset: /home/kai/Desktop/slr/data/ds/vanilla/normal/train_prior_100.pkl
  action_dim: 2
  action_mag: 1.0
  use_append: true
  obs_buffer: 0.0
  g_x_fail: 1.0
  terminal_type: const
  img_h: 48
  img_w: 48
  max_step_train: 200
  max_step_eval: 200
  obs_channel: 3
  fixed_init: true
  sparse_reward: false
  num_env_train: 100
  reward_type: task
  reward_goal: 10
  reward_wander: 2
  reward_obs: -10

agent:
  use_wandb: true
  device: cuda:0
  image_device: cpu
  num_cpus: 8
  num_gpu_envs: null
  cpu_offset: 0
  seed: 0
  entity: pac-saferl
  project: prior_vanilla
  run: pac_perf  # wandb name
  out_folder: /home/kai/Desktop/slr/checkpoint/vanilla/sim_pac_perf/0
  name: PolicyPriorPerfLatent  # agent name
  save_top_k: 20
  save_metric: value
  max_steps: 500000
  memory_capacity: 50000
  check_opt_freq: 5
  min_steps_b4_opt: 2000
  optimize_freq: 2000
  update_per_opt: 1000
  disc_update_per_opt: 100
  num_visualize_task: 3
  check_type: all_env
  num_traj_per_env: 1
  # num_validation_trajs: 200
  plot_v: true
  random_init: true
  #
  eta: 0.0
  eta_decay: 0.1
  eta_end: 0.0  #! shuts down the eta scheduler.
  eta_period: 100000
  #
  eps: 0.0
  eps_decay: 0.5
  eps_end: 1.0
  eps_period: 50000
  #
  rho: 1.0
  rho_decay: 0.5
  rho_period: 25000
  rho_traj: false
  #
  use_shielding: true
  train_shield_dict:
    type: value
    threshold: -0.05
  value_shield_dict:
    type: value
    threshold: -0.05
  #
  use_append_noise: false
  action_relabel: true
  perf_diversity_type: state

performance:
  train:
    eval: false
    device: cuda:0
    batch_size: 128
    update_period: 1
    alpha: 0.1
    learn_alpha: true
    gamma: 0.99
    gamma_schedule: false
    lr_a: 0.0001
    lr_al: 0.0001
    lr_c: 0.0001
    lr_a_schedule: false
    lr_al_schedule: false
    lr_c_schedule: false
    mode: performance
    tau: 0.01
    terminal_type: none
    # latent
    lr_d: 0.0001
    disc_batch_size: 32
    disc_recent_size: 10000
    aug_reward_range_schedule: false
    aug_reward_range: 2.0
    latent_dim: 20
    latent_dim_cnn: 0
    latent_std_schedule: false
    latent_prior_std: 2.0
  arch:
    critic_has_act_ind: false
    act_ind:
      - 1
    activation:
      actor: ReLU
      critic: ReLU
    kernel_size:
    - 5
    - 3
    - 3
    stride:
    - 2
    - 2
    - 2
    n_channel:
    - 8
    - 16
    - 32
    mlp_dim:
      actor:
      - 128
      - 128
      critic:
      - 128
      - 128
      disc:
      - 128
      - 128
    append_dim: 2
    use_bn: false
    use_ln: true
    use_sm: false
    use_spec_disc: true

backup:
  train:
    eval: false
    device: cuda:0
    alpha: 0.1
    learn_alpha: true
    batch_size: 128
    gamma: 0.8
    gamma_decay: 0.5
    gamma_end: 0.999
    gamma_period: 25000
    gamma_schedule: true
    lr_a: 0.0001
    lr_al: 0.0001
    lr_c: 0.0001
    lr_a_schedule: false
    lr_al_schedule: false
    lr_c_schedule: false
    mode: safety
    tau: 0.01
    terminal_type: none
    # latent
    latent_dim: 0
    latent_dim_cnn: 0
  arch:
    critic_has_act_ind: true
    act_ind:
      - -1
    activation:
      actor: ReLU
      critic: ReLU
    kernel_size:
    - 5
    - 3
    - 3
    stride:
    - 2
    - 2
    - 2
    n_channel:
    - 8
    - 16
    - 32
    mlp_dim:
      actor:
      - 128
      - 128
      critic:
      - 128
      - 128
    append_dim: 2
    use_bn: false
    use_ln: true
    use_sm: false
